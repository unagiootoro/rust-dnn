mod test_utils;

use crate::test_utils::{arange_with_shape, assert_tensor, assert_tensor_with_eps};
use rust_dnn_core::{
    backend::Backend,
    device::{self, Device},
    error::Result,
    ten,
    tensor::Tensor,
};
use rust_dnn_nn::layer::{BatchNorm1d, BatchNorm2d, Conv2D, Deconv2D, Linear, layer_norm};

fn test_linear_forward<B: Backend>(device: Device<B>) -> Result<()> {
    let x = ten![[0.0, -2.0, -4.0], [1.0, 2.0, 3.0]]
        .to_device(device)?
        .requires_grad();
    let weight = ten![[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
        .to_device(device)?
        .requires_grad();
    let bias = ten![1.0, 2.0].to_device(device)?.requires_grad();
    let linear = Linear::from_weights(weight, Some(bias))?;
    let y = linear.forward(&x)?;
    assert_tensor(&y, &ten![[-15.0, -32.0], [15.0, 34.0]]);
    Ok(())
}

define_test!(
    test_linear_forward,
    test_linear_forward_cpu,
    test_linear_forward_cuda
);

fn test_linear_no_bias_forward<B: Backend>(device: Device<B>) -> Result<()> {
    let x: rust_dnn_core::tensor::Tensor<B, f64> = ten![[0.0, -2.0, -4.0], [1.0, 2.0, 3.0]]
        .to_device(device)?
        .requires_grad();
    let weight = ten![[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
        .to_device(device)?
        .requires_grad();
    let linear = Linear::from_weights(weight, None)?;
    let y = linear.forward(&x)?;
    assert_tensor(&y, &ten![[-16.0, -34.0], [14.0, 32.0]]);
    Ok(())
}

define_test!(
    test_linear_no_bias_forward,
    test_linear_no_bias_forward_cpu,
    test_linear_no_bias_forward_cuda
);

fn test_linear_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let x = ten![[0.0, -2.0, -4.0], [1.0, 2.0, 3.0]]
        .to_device(device)?
        .requires_grad();
    let weight = ten![[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
        .to_device(device)?
        .requires_grad();
    let bias = ten![1.0, 2.0].to_device(device)?.requires_grad();
    let linear = Linear::from_weights(weight.clone(), Some(bias.clone()))?;
    let y = linear.forward(&x)?;
    assert_tensor(&y, &ten![[-15.0, -32.0], [15.0, 34.0]]);
    let grads = y.backward()?;
    let gx = grads.get(&x).unwrap();
    let gw = grads.get(&weight).unwrap();
    let gb = grads.get(&bias).unwrap();
    assert_tensor(&gx, &ten![[5.0, 7.0, 9.0], [5.0, 7.0, 9.0]]);
    assert_tensor(&gw, &ten![[1.0, 0.0, -1.0], [1.0, 0.0, -1.0]]);
    assert_tensor(&gb, &ten![2.0, 2.0]);
    Ok(())
}

define_test!(
    test_linear_backward,
    test_linear_backward_cpu,
    test_linear_backward_cuda
);

fn test_linear_no_bias_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let x = ten![[0.0, -2.0, -4.0], [1.0, 2.0, 3.0]]
        .to_device(device)?
        .requires_grad();
    let weight = ten![[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
        .to_device(device)?
        .requires_grad();
    let linear = Linear::from_weights(weight.clone(), None)?;
    let y = linear.forward(&x)?;
    assert_tensor(&y, &ten![[-16.0, -34.0], [14.0, 32.0]]);
    let grads = y.backward()?;
    let gx = grads.get(&x).unwrap();
    let gw = grads.get(&weight).unwrap();
    assert_tensor(&gx, &ten![[5.0, 7.0, 9.0], [5.0, 7.0, 9.0]]);
    assert_tensor(&gw, &ten![[1.0, 0.0, -1.0], [1.0, 0.0, -1.0]]);
    Ok(())
}

define_test!(
    test_linear_no_bias_backward,
    test_linear_no_bias_backward_cpu,
    test_linear_no_bias_backward_cuda
);

fn test_conv2d<B: Backend>(device: Device<B>) -> Result<()> {
    let batch_size = 2;
    let in_filters = 3;
    let out_filters = 2;
    let img_h = 4;
    let img_w = 5;
    let fil_h = 2;
    let fil_w = 3;
    let x = arange_with_shape::<_, f64>(&[batch_size, in_filters, img_h, img_w], device);
    let w = arange_with_shape(&[out_filters, in_filters, fil_h, fil_w], device);
    let b = arange_with_shape(&[out_filters], device);

    let conv2d = Conv2D::new(
        in_filters,
        out_filters,
        fil_h,
        fil_w,
        1,
        1,
        None,
        false,
        true,
        device,
    );

    conv2d.weight().copy(&w)?;
    conv2d.bias().unwrap().copy(&b)?;

    let y = conv2d.forward(&x)?;
    assert_eq!(y.shape(), &vec![batch_size, out_filters, 3, 3]);
    assert_eq!(
        y.to_vec(),
        vec![
            5115., 5268., 5421., 5880., 6033., 6186., 6645., 6798., 6951., 12730., 13207., 13684.,
            15115., 15592., 16069., 17500., 17977., 18454., 14295., 14448., 14601., 15060., 15213.,
            15366., 15825., 15978., 16131., 41350., 41827., 42304., 43735., 44212., 44689., 46120.,
            46597., 47074.,
        ]
    );
    Ok(())
}

define_test!(test_conv2d, test_conv2d_cpu, test_conv2d_cuda);

fn test_deconv2d<B: Backend>(device: Device<B>) -> Result<()> {
    let batch_size = 2;
    let in_filters = 2;
    let out_filters = 3;
    let img_h = 3;
    let img_w = 3;
    let fil_h = 2;
    let fil_w = 3;
    let x = arange_with_shape::<_, f64>(&[batch_size, in_filters, img_h, img_w], device);
    let w = arange_with_shape(&[in_filters, out_filters, fil_h, fil_w], device);
    let b = arange_with_shape(&[out_filters], device);

    let deconv2d = Deconv2D::new(
        in_filters,
        out_filters,
        fil_h,
        fil_w,
        1,
        1,
        None,
        false,
        true,
        device,
    );

    deconv2d.weight().copy(&w)?;
    deconv2d.bias().unwrap().copy(&b)?;

    let y = deconv2d.forward(&x)?;
    assert_eq!(y.shape(), &vec![batch_size, out_filters, 4, 5]);
    assert_eq!(
        y.to_vec(),
        vec![
            162., 351., 569., 413., 224., 405., 876., 1417., 1024., 553., 531., 1140., 1831.,
            1312., 703., 333., 711., 1136., 809., 431., 217., 472., 768., 558., 303., 550., 1189.,
            1922., 1385., 746., 748., 1597., 2552., 1817., 968., 460., 976., 1551., 1098., 582.,
            272., 593., 967., 703., 382., 695., 1502., 2427., 1746., 939., 965., 2054., 3273.,
            2322., 1233., 587., 1241., 1966., 1387., 733., 486., 1035., 1649., 1169., 620., 1161.,
            2460., 3901., 2752., 1453., 1287., 2724., 4315., 3040., 1603., 765., 1611., 2540.,
            1781., 935., 757., 1588., 2496., 1746., 915., 1738., 3637., 5702., 3977., 2078., 1936.,
            4045., 6332., 4409., 2300., 1108., 2308., 3603., 2502., 1302., 1028., 2141., 3343.,
            2323., 1210., 2315., 4814., 7503., 5202., 2703., 2585., 5366., 8349., 5778., 2997.,
            1451., 3005., 4666., 3223., 1669.,
        ]
    );
    Ok(())
}

define_test!(test_deconv2d, test_deconv2d_cpu, test_deconv2d_cuda);

fn test_batch_norm1d<B: Backend>(device: Device<B>) -> Result<()> {
    let x = Tensor::from_vec(
        vec![1.0, 5.0, 3.0, 2.0, 1.0, 6.0, 4.0, 2.0, 1.0, 3.0, 7.0, 8.0],
        vec![4, 3],
        device,
    )?
    .requires_grad();
    let gamma = Tensor::from_vec(vec![1.0, 1.5, 2.0], vec![3], device)?.requires_grad();
    let beta = Tensor::from_vec(vec![0.0, 0.5, -1.0], vec![3], device)?.requires_grad();
    let mut batch_norm1d = BatchNorm1d::new(3, 0.9, 1e-7, device);
    batch_norm1d.gamma().copy(&gamma)?;
    batch_norm1d.beta().copy(&beta)?;
    let y = batch_norm1d.forward(&x, true)?;
    assert_tensor(
        &y,
        &ten![
            [-1.3416, 1.2862, -2.1142],
            [-0.4472, -1.2297, 0.1142],
            [1.3416, -0.6007, -3.5997],
            [0.4472, 2.5442, 1.5997,]
        ],
    );
    Ok(())
}

define_test!(
    test_batch_norm1d,
    test_batch_norm1d_cpu,
    test_batch_norm1d_cuda
);

fn test_batch_norm1d_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let x = Tensor::from_vec(
        vec![1.0, 5.0, 3.0, 2.0, 1.0, 6.0, 4.0, 2.0, 1.0, 3.0, 7.0, 8.0],
        vec![4, 3],
        device,
    )?
    .requires_grad();
    let gamma = Tensor::from_vec(vec![1.0, 1.5, 2.0], vec![3], device)?;
    let beta = Tensor::from_vec(vec![0.0, 0.5, -1.0], vec![3], device)?;
    let mut batch_norm1d = BatchNorm1d::new(3, 0.9, 1e-7, device);
    batch_norm1d.gamma().copy(&gamma)?;
    batch_norm1d.beta().copy(&beta)?;
    let y = batch_norm1d.forward(&x, true)?;
    assert_tensor(
        &y,
        &ten![
            [-1.3416, 1.2862, -2.1142],
            [-0.4472, -1.2297, 0.1142],
            [1.3416, -0.6007, -3.5997],
            [0.4472, 2.5442, 1.5997,]
        ],
    );

    let x2 = Tensor::from_vec(
        vec![
            1.0, 0.0, -1.0, -1.0, 2.0, 0.0, 0.5, -0.5, 1.0, -1.5, 1.0, -0.5,
        ],
        vec![4, 3],
        device,
    )?;
    let y = (&y * x2)?;

    let grads = y.backward()?;
    let gx = grads.get(&x).unwrap();
    let ggamma = grads.get(&batch_norm1d.gamma()).unwrap();
    let gbeta = grads.get(&batch_norm1d.beta()).unwrap();
    assert_tensor(
        &gx,
        &ten![
            [0.8497, -0.3456, -0.7940],
            [-0.7603, 0.7603, 0.2369],
            [0.9391, -0.7741, 0.4995],
            [-1.0286, 0.3594, 0.0576],
        ],
    );
    assert_tensor(&ggamma, &ten![-0.8944, -0.5766, -1.3927]);
    assert_tensor(&gbeta, &ten![-1.0000, 2.5000, -0.5000]);
    Ok(())
}

define_test!(
    test_batch_norm1d_backward,
    test_batch_norm1d_backward_cpu,
    test_batch_norm1d_backward_cuda
);

static BATCH_NORM2D_FORWARD_EXPECTED_DATA: [f64; 360] = [
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.6848, -0.1072, 0.4705, 1.0481, 1.6258,
    2.2034, -0.5693, 0.0084, 0.5860, 1.1637, 1.7413, 2.3190, -0.4538, 0.1239, 0.7015, 1.2792,
    1.8569, 2.4345, -0.3382, 0.2394, 0.8171, 1.3947, 1.9724, 2.5500, -0.2227, 0.3550, 0.9326,
    1.5103, 2.0879, 2.6656, -1.3696, -0.2143, 0.9410, 2.0963, 3.2516, 4.4069, -1.1386, 0.0167,
    1.1720, 2.3273, 3.4826, 4.6380, -0.9075, 0.2478, 1.4031, 2.5584, 3.7137, 4.8690, -0.6765,
    0.4788, 1.6342, 2.7895, 3.9448, 5.1001, -0.4454, 0.7099, 1.8652, 3.0205, 4.1758, 5.3311,
    -2.0545, -0.3215, 1.4115, 3.1444, 4.8774, 6.6103, -1.7079, 0.0251, 1.7580, 3.4910, 5.2240,
    6.9569, -1.3613, 0.3717, 2.1046, 3.8376, 5.5706, 7.3035, -1.0147, 0.7183, 2.4512, 4.1842,
    5.9171, 7.6501, -0.6681, 1.0649, 2.7978, 4.5308, 6.2637, 7.9967, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, -0.6752, -0.0975, 0.4801, 1.0578, 1.6354, 2.2131, -0.5597, 0.0180,
    0.5956, 1.1733, 1.7509, 2.3286, -0.4441, 0.1335, 0.7112, 1.2888, 1.8665, 2.4441, -0.3286,
    0.2491, 0.8267, 1.4044, 1.9820, 2.5597, -0.2131, 0.3646, 0.9422, 1.5199, 2.0975, 2.6752,
    -1.3504, -0.1951, 0.9602, 2.1155, 3.2708, 4.4261, -1.1193, 0.0360, 1.1913, 2.3466, 3.5019,
    4.6572, -0.8883, 0.2670, 1.4223, 2.5777, 3.7330, 4.8883, -0.6572, 0.4981, 1.6534, 2.8087,
    3.9640, 5.1193, -0.4261, 0.7292, 1.8845, 3.0398, 4.1951, 5.3504, -2.0256, -0.2926, 1.4403,
    3.1733, 4.9063, 6.6392, -1.6790, 0.0540, 1.7869, 3.5199, 5.2528, 6.9858, -1.3324, 0.4006,
    2.1335, 3.8665, 5.5994, 7.3324, -0.9858, 0.7472, 2.4801, 4.2131, 5.9460, 7.6790, -0.6392,
    1.0937, 2.8267, 4.5597, 6.2926, 8.0256, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    -0.6656, -0.0879, 0.4897, 1.0674, 1.6450, 2.2227, -0.5500, 0.0276, 0.6053, 1.1829, 1.7606,
    2.3382, -0.4345, 0.1431, 0.7208, 1.2985, 1.8761, 2.4538, -0.3190, 0.2587, 0.8363, 1.4140,
    1.9916, 2.5693, -0.2034, 0.3742, 0.9519, 1.5295, 2.1072, 2.6848, -1.3311, -0.1758, 0.9795,
    2.1348, 3.2901, 4.4454, -1.1001, 0.0552, 1.2105, 2.3658, 3.5212, 4.6765, -0.8690, 0.2863,
    1.4416, 2.5969, 3.7522, 4.9075, -0.6380, 0.5174, 1.6727, 2.8280, 3.9833, 5.1386, -0.4069,
    0.7484, 1.9037, 3.0590, 4.2143, 5.3696, -1.9967, -0.2637, 1.4692, 3.2022, 4.9351, 6.6681,
    -1.6501, 0.0829, 1.8158, 3.5488, 5.2817, 7.0147, -1.3035, 0.4294, 2.1624, 3.8954, 5.6283,
    7.3613, -0.9569, 0.7760, 2.5090, 4.2420, 5.9749, 7.7079, -0.6103, 1.1226, 2.8556, 4.5885,
    6.3215, 8.0545,
];

static BATCH_NORM2D_FORWARD_PREDICT_EXPECTED_DATA: [f64; 360] = [
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5496, 2.3755, 4.2015, 6.0274, 7.8534, 9.6793,
    0.9148, 2.7407, 4.5667, 6.3926, 8.2185, 10.0445, 1.2800, 3.1059, 4.9319, 6.7578, 8.5837,
    10.4097, 1.6452, 3.4711, 5.2970, 7.1230, 8.9489, 10.7749, 2.0104, 3.8363, 5.6622, 7.4882,
    9.3141, 11.1400, 1.2635, 4.9154, 8.5673, 12.2192, 15.8711, 19.5229, 1.9939, 5.6458, 9.2977,
    12.9495, 16.6014, 20.2533, 2.7243, 6.3762, 10.0280, 13.6799, 17.3318, 20.9837, 3.4547, 7.1065,
    10.7584, 14.4103, 18.0622, 21.7141, 4.1850, 7.8369, 11.4888, 15.1407, 18.7926, 22.4444, 2.1418,
    7.6196, 13.0974, 18.5753, 24.0531, 29.5309, 3.2374, 8.7152, 14.1930, 19.6708, 25.1486, 30.6265,
    4.3329, 9.8108, 15.2886, 20.7664, 26.2442, 31.7220, 5.4285, 10.9063, 16.3841, 21.8620, 27.3398,
    32.8176, 6.5241, 12.0019, 17.4797, 22.9575, 28.4353, 33.9132, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.5800, 2.4060, 4.2319, 6.0579, 7.8838, 9.7097, 0.9452, 2.7712, 4.5971, 6.4230,
    8.2490, 10.0749, 1.3104, 3.1363, 4.9623, 6.7882, 8.6142, 10.4401, 1.6756, 3.5015, 5.3275,
    7.1534, 8.9794, 10.8053, 2.0408, 3.8667, 5.6927, 7.5186, 9.3445, 11.1705, 1.3244, 4.9763,
    8.6282, 12.2800, 15.9319, 19.5838, 2.0548, 5.7067, 9.3585, 13.0104, 16.6623, 20.3142, 2.7852,
    6.4370, 10.0889, 13.7408, 17.3927, 21.0445, 3.5155, 7.1674, 10.8193, 14.4712, 18.1230, 21.7749,
    4.2459, 7.8978, 11.5497, 15.2015, 18.8534, 22.5053, 2.2331, 7.7109, 13.1887, 18.6666, 24.1444,
    29.6222, 3.3287, 8.8065, 14.2843, 19.7621, 25.2399, 30.7178, 4.4242, 9.9021, 15.3799, 20.8577,
    26.3355, 31.8133, 5.5198, 10.9976, 16.4754, 21.9532, 27.4311, 32.9089, 6.6154, 12.0932,
    17.5710, 23.0488, 28.5266, 34.0044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6105,
    2.4364, 4.2623, 6.0883, 7.9142, 9.7402, 0.9757, 2.8016, 4.6275, 6.4535, 8.2794, 10.1054,
    1.3408, 3.1668, 4.9927, 6.8187, 8.6446, 10.4705, 1.7060, 3.5320, 5.3579, 7.1838, 9.0098,
    10.8357, 2.0712, 3.8972, 5.7231, 7.5490, 9.3750, 11.2009, 1.3853, 5.0371, 8.6890, 12.3409,
    15.9928, 19.6447, 2.1156, 5.7675, 9.4194, 13.0713, 16.7232, 20.3750, 2.8460, 6.4979, 10.1498,
    13.8017, 17.4535, 21.1054, 3.5764, 7.2283, 10.8802, 14.5320, 18.1839, 21.8358, 4.3068, 7.9586,
    11.6105, 15.2624, 18.9143, 22.5662, 2.3244, 7.8022, 13.2800, 18.7579, 24.2357, 29.7135, 3.4200,
    8.8978, 14.3756, 19.8534, 25.3312, 30.8091, 4.5155, 9.9933, 15.4712, 20.9490, 26.4268, 31.9046,
    5.6111, 11.0889, 16.5667, 22.0445, 27.5224, 33.0002, 6.7067, 12.1845, 17.6623, 23.1401,
    28.6179, 34.0957,
];

static BATCH_NORM2D_BACKWARD_DATA_EXPECTED_DATA: [f64; 360] = [
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -1.2326, -1.2443, -1.2561, -1.2678, -1.2795,
    -1.2913, -1.1791, -1.1908, -1.2026, -1.2143, -1.2260, -1.2378, -1.1256, -1.1373, -1.1491,
    -1.1608, -1.1726, -1.1843, -1.0721, -1.0839, -1.0956, -1.1073, -1.1191, -1.1308, -1.0186,
    -1.0304, -1.0421, -1.0538, -1.0656, -1.0773, -2.4652, -2.4887, -2.5121, -2.5356, -2.5591,
    -2.5826, -2.3582, -2.3817, -2.4052, -2.4286, -2.4521, -2.4756, -2.2512, -2.2747, -2.2982,
    -2.3216, -2.3451, -2.3686, -2.1442, -2.1677, -2.1912, -2.2147, -2.2381, -2.2616, -2.0373,
    -2.0607, -2.0842, -2.1077, -2.1311, -2.1546, -3.6978, -3.7330, -3.7682, -3.8034, -3.8386,
    -3.8738, -3.5373, -3.5725, -3.6077, -3.6429, -3.6781, -3.7133, -3.3768, -3.4120, -3.4472,
    -3.4825, -3.5177, -3.5529, -3.2164, -3.2516, -3.2868, -3.3220, -3.3572, -3.3924, -3.0559,
    -3.0911, -3.1263, -3.1615, -3.1967, -3.2319, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    -0.0776, -0.0894, -0.1011, -0.1129, -0.1246, -0.1363, -0.0242, -0.0359, -0.0476, -0.0594,
    -0.0711, -0.0828, 0.0293, 0.0176, 0.0059, -0.0059, -0.0176, -0.0293, 0.0828, 0.0711, 0.0594,
    0.0476, 0.0359, 0.0242, 0.1363, 0.1246, 0.1129, 0.1011, 0.0894, 0.0776, -0.1553, -0.1788,
    -0.2022, -0.2257, -0.2492, -0.2727, -0.0483, -0.0718, -0.0952, -0.1187, -0.1422, -0.1657,
    0.0587, 0.0352, 0.0117, -0.0117, -0.0352, -0.0587, 0.1657, 0.1422, 0.1187, 0.0952, 0.0718,
    0.0483, 0.2727, 0.2492, 0.2257, 0.2022, 0.1788, 0.1553, -0.2329, -0.2681, -0.3034, -0.3386,
    -0.3738, -0.4090, -0.0725, -0.1077, -0.1429, -0.1781, -0.2133, -0.2485, 0.0880, 0.0528, 0.0176,
    -0.0176, -0.0528, -0.0880, 0.2485, 0.2133, 0.1781, 0.1429, 0.1077, 0.0725, 0.4090, 0.3738,
    0.3386, 0.3034, 0.2681, 0.2329, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0773, 1.0656,
    1.0538, 1.0421, 1.0304, 1.0186, 1.1308, 1.1191, 1.1073, 1.0956, 1.0839, 1.0721, 1.1843, 1.1726,
    1.1608, 1.1491, 1.1373, 1.1256, 1.2378, 1.2260, 1.2143, 1.2026, 1.1908, 1.1791, 1.2913, 1.2795,
    1.2678, 1.2561, 1.2443, 1.2326, 2.1546, 2.1311, 2.1077, 2.0842, 2.0607, 2.0373, 2.2616, 2.2381,
    2.2147, 2.1912, 2.1677, 2.1442, 2.3686, 2.3451, 2.3216, 2.2982, 2.2747, 2.2512, 2.4756, 2.4521,
    2.4286, 2.4052, 2.3817, 2.3582, 2.5826, 2.5591, 2.5356, 2.5121, 2.4887, 2.4652, 3.2319, 3.1967,
    3.1615, 3.1263, 3.0911, 3.0559, 3.3924, 3.3572, 3.3220, 3.2868, 3.2516, 3.2164, 3.5529, 3.5177,
    3.4825, 3.4472, 3.4120, 3.3768, 3.7134, 3.6781, 3.6429, 3.6077, 3.5725, 3.5373, 3.8738, 3.8386,
    3.8034, 3.7682, 3.7330, 3.6978,
];

static BATCH_NORM2D_BACKWARD_GAMMA_EXPECTED_DATA: [f64; 4] =
    [345.7256, 345.7256, 345.7256, 345.7257];
static BATCH_NORM2D_BACKWARD_BETA_EXPECTED_DATA: [f64; 4] = [12105., 14805., 17505., 20205.];

fn test_batch_norm2d<B: Backend>(device: Device<B>) -> Result<()> {
    let n = 3;
    let c = 4;
    let h = 5;
    let w = 6;
    let x = arange_with_shape::<_, f64>(&[w, h, c, n], device);
    let x = x.reversed_axes()?;
    let gamma = arange_with_shape(&[c], device);
    let beta = arange_with_shape(&[c], device);
    let mut batch_norm2d = BatchNorm2d::new(c, 0.9, 1e-7, device);
    batch_norm2d.gamma().copy(&gamma)?;
    batch_norm2d.beta().copy(&beta)?;
    let y = batch_norm2d.forward(&x, true)?;
    let expected = Tensor::from_vec(
        BATCH_NORM2D_FORWARD_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor(&y, &expected);
    Ok(())
}

define_test!(
    test_batch_norm2d,
    test_batch_norm2d_cpu,
    test_batch_norm2d_cuda
);

fn test_batch_norm2d_predict<B: Backend>(device: Device<B>) -> Result<()> {
    let n = 3;
    let c = 4;
    let h = 5;
    let w = 6;
    let x = arange_with_shape::<_, f64>(&[w, h, c, n], device);
    let x = x.reversed_axes()?.requires_grad();
    let gamma = arange_with_shape(&[c], device);
    let beta = arange_with_shape(&[c], device);
    let mut batch_norm2d = BatchNorm2d::new(c, 0.9, 1e-7, device);
    batch_norm2d.gamma().copy(&gamma)?;
    batch_norm2d.beta().copy(&beta)?;
    let y = batch_norm2d.forward(&x, true)?;
    let expected_y = Tensor::from_vec(
        BATCH_NORM2D_FORWARD_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor(&y, &expected_y);

    let x2 = arange_with_shape(&[n, c, h, w], device);
    let y2 = (y * x2)?;

    let grads: rust_dnn_core::gradients::Gradients<B, f64> = y2.backward()?;
    let gx = grads.get(&x).unwrap();
    let ggamma = grads.get(&batch_norm2d.gamma()).unwrap();
    let gbeta = grads.get(&batch_norm2d.beta()).unwrap();

    let expected_gx = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_DATA_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor(&gx, &expected_gx);

    let expected_ggamma = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_GAMMA_EXPECTED_DATA.to_vec(),
        vec![c],
        device,
    )?;
    assert_tensor_with_eps(&ggamma, &expected_ggamma, 1e-3);

    let expected_gbeta = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_BETA_EXPECTED_DATA.to_vec(),
        vec![c],
        device,
    )?;
    assert_tensor(&gbeta, &expected_gbeta);

    let y3 = batch_norm2d.forward(&x, false)?;
    let expected_y3 = Tensor::from_vec(
        BATCH_NORM2D_FORWARD_PREDICT_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&y3, &expected_y3, 1e-1);

    Ok(())
}

define_test!(
    test_batch_norm2d_predict,
    test_batch_norm2d_predict_cpu,
    test_batch_norm2d_predict_cuda
);

fn test_batch_norm2d_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let n = 3;
    let c = 4;
    let h = 5;
    let w = 6;
    let x = arange_with_shape::<_, f64>(&[w, h, c, n], device);
    let x = x.reversed_axes()?.requires_grad();
    let gamma = arange_with_shape(&[c], device);
    let beta = arange_with_shape(&[c], device);
    let mut batch_norm2d = BatchNorm2d::new(c, 0.9, 1e-7, device);
    batch_norm2d.gamma().copy(&gamma)?;
    batch_norm2d.beta().copy(&beta)?;
    let y = batch_norm2d.forward(&x, true)?;
    let expected_y = Tensor::from_vec(
        BATCH_NORM2D_FORWARD_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor(&y, &expected_y);

    let x2 = arange_with_shape(&[n, c, h, w], device);
    let y2 = (y * x2)?;

    let grads: rust_dnn_core::gradients::Gradients<B, f64> = y2.backward()?;
    let gx = grads.get(&x).unwrap();
    let ggamma = grads.get(&batch_norm2d.gamma()).unwrap();
    let gbeta = grads.get(&batch_norm2d.beta()).unwrap();

    let expected_gx = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_DATA_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor(&gx, &expected_gx);

    let expected_ggamma = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_GAMMA_EXPECTED_DATA.to_vec(),
        vec![c],
        device,
    )?;
    assert_tensor_with_eps(&ggamma, &expected_ggamma, 1e-3);

    let expected_gbeta = Tensor::from_vec(
        BATCH_NORM2D_BACKWARD_BETA_EXPECTED_DATA.to_vec(),
        vec![c],
        device,
    )?;
    assert_tensor(&gbeta, &expected_gbeta);

    Ok(())
}

define_test!(
    test_batch_norm2d_backward,
    test_batch_norm2d_backward_cpu,
    test_batch_norm2d_backward_cuda
);

static LAYER_NORM_FORWARD_EXPECTED_DATA: [f32; 360] = [
    0.0000e+00,
    -1.4031e-01,
    8.7413e-01,
    3.0433e+00,
    6.3672e+00,
    1.0846e+01,
    -3.6132e+00,
    -1.7382e-01,
    4.4203e+00,
    1.0169e+01,
    1.7073e+01,
    2.5131e+01,
    -5.8407e+00,
    1.1783e+00,
    9.3522e+00,
    1.8681e+01,
    2.9164e+01,
    4.0802e+01,
    -6.6826e+00,
    3.9162e+00,
    1.5670e+01,
    2.8578e+01,
    4.2641e+01,
    5.7859e+01,
    -6.1387e+00,
    8.0398e+00,
    2.3373e+01,
    3.9861e+01,
    5.7504e+01,
    7.6301e+01,
    -2.0664e+01,
    -3.4546e+00,
    1.4910e+01,
    3.4429e+01,
    5.5103e+01,
    7.6932e+01,
    -2.0640e+01,
    1.4935e-01,
    2.2093e+01,
    4.5192e+01,
    6.9446e+01,
    9.4854e+01,
    -1.9230e+01,
    5.1389e+00,
    3.0663e+01,
    5.7341e+01,
    8.5175e+01,
    1.1416e+02,
    -1.6435e+01,
    1.1514e+01,
    4.0618e+01,
    7.0876e+01,
    1.0229e+02,
    1.3486e+02,
    -1.2253e+01,
    1.9275e+01,
    5.1958e+01,
    8.5796e+01,
    1.2079e+02,
    1.5694e+02,
    -3.9596e+01,
    -5.0367e+00,
    3.0678e+01,
    6.7547e+01,
    1.0557e+02,
    1.4475e+02,
    -3.5935e+01,
    2.2046e+00,
    4.1499e+01,
    8.1948e+01,
    1.2355e+02,
    1.6631e+02,
    -3.0887e+01,
    1.0832e+01,
    5.3705e+01,
    9.7734e+01,
    1.4292e+02,
    1.8926e+02,
    -2.4454e+01,
    2.0844e+01,
    6.7298e+01,
    1.1491e+02,
    1.6367e+02,
    2.1359e+02,
    -1.6636e+01,
    3.2243e+01,
    8.2276e+01,
    1.3346e+02,
    1.8581e+02,
    2.3930e+02,
    -5.6796e+01,
    -4.8868e+00,
    4.8178e+01,
    1.0240e+02,
    1.5777e+02,
    2.1430e+02,
    -4.9497e+01,
    5.9920e+00,
    6.2636e+01,
    1.2043e+02,
    1.7939e+02,
    2.3950e+02,
    -4.0813e+01,
    1.8256e+01,
    7.8480e+01,
    1.3986e+02,
    2.0239e+02,
    2.6608e+02,
    -3.0742e+01,
    3.1907e+01,
    9.5710e+01,
    1.6067e+02,
    2.2678e+02,
    2.9405e+02,
    -1.9286e+01,
    4.6942e+01,
    1.1433e+02,
    1.8286e+02,
    2.5256e+02,
    3.2340e+02,
    0.0000e+00,
    -1.4031e-01,
    8.7413e-01,
    3.0433e+00,
    6.3672e+00,
    1.0846e+01,
    -3.6132e+00,
    -1.7382e-01,
    4.4203e+00,
    1.0169e+01,
    1.7073e+01,
    2.5131e+01,
    -5.8407e+00,
    1.1783e+00,
    9.3522e+00,
    1.8681e+01,
    2.9164e+01,
    4.0802e+01,
    -6.6826e+00,
    3.9162e+00,
    1.5670e+01,
    2.8578e+01,
    4.2641e+01,
    5.7859e+01,
    -6.1387e+00,
    8.0398e+00,
    2.3373e+01,
    3.9861e+01,
    5.7504e+01,
    7.6301e+01,
    -2.0664e+01,
    -3.4546e+00,
    1.4910e+01,
    3.4429e+01,
    5.5103e+01,
    7.6932e+01,
    -2.0640e+01,
    1.4935e-01,
    2.2093e+01,
    4.5192e+01,
    6.9446e+01,
    9.4854e+01,
    -1.9230e+01,
    5.1389e+00,
    3.0663e+01,
    5.7341e+01,
    8.5175e+01,
    1.1416e+02,
    -1.6435e+01,
    1.1514e+01,
    4.0618e+01,
    7.0876e+01,
    1.0229e+02,
    1.3486e+02,
    -1.2253e+01,
    1.9275e+01,
    5.1958e+01,
    8.5796e+01,
    1.2079e+02,
    1.5694e+02,
    -3.9596e+01,
    -5.0367e+00,
    3.0678e+01,
    6.7547e+01,
    1.0557e+02,
    1.4475e+02,
    -3.5935e+01,
    2.2046e+00,
    4.1499e+01,
    8.1948e+01,
    1.2355e+02,
    1.6631e+02,
    -3.0887e+01,
    1.0832e+01,
    5.3705e+01,
    9.7734e+01,
    1.4292e+02,
    1.8926e+02,
    -2.4454e+01,
    2.0844e+01,
    6.7298e+01,
    1.1491e+02,
    1.6367e+02,
    2.1359e+02,
    -1.6636e+01,
    3.2243e+01,
    8.2276e+01,
    1.3346e+02,
    1.8581e+02,
    2.3930e+02,
    -5.6796e+01,
    -4.8868e+00,
    4.8178e+01,
    1.0240e+02,
    1.5777e+02,
    2.1430e+02,
    -4.9497e+01,
    5.9920e+00,
    6.2636e+01,
    1.2043e+02,
    1.7939e+02,
    2.3950e+02,
    -4.0813e+01,
    1.8256e+01,
    7.8480e+01,
    1.3986e+02,
    2.0239e+02,
    2.6608e+02,
    -3.0742e+01,
    3.1907e+01,
    9.5710e+01,
    1.6067e+02,
    2.2678e+02,
    2.9405e+02,
    -1.9286e+01,
    4.6942e+01,
    1.1433e+02,
    1.8286e+02,
    2.5256e+02,
    3.2340e+02,
    0.0000e+00,
    -1.4031e-01,
    8.7413e-01,
    3.0433e+00,
    6.3672e+00,
    1.0846e+01,
    -3.6132e+00,
    -1.7382e-01,
    4.4203e+00,
    1.0169e+01,
    1.7073e+01,
    2.5131e+01,
    -5.8407e+00,
    1.1783e+00,
    9.3522e+00,
    1.8681e+01,
    2.9164e+01,
    4.0802e+01,
    -6.6826e+00,
    3.9162e+00,
    1.5670e+01,
    2.8578e+01,
    4.2641e+01,
    5.7859e+01,
    -6.1387e+00,
    8.0398e+00,
    2.3373e+01,
    3.9861e+01,
    5.7504e+01,
    7.6301e+01,
    -2.0664e+01,
    -3.4546e+00,
    1.4910e+01,
    3.4429e+01,
    5.5103e+01,
    7.6932e+01,
    -2.0640e+01,
    1.4935e-01,
    2.2093e+01,
    4.5192e+01,
    6.9446e+01,
    9.4854e+01,
    -1.9230e+01,
    5.1389e+00,
    3.0663e+01,
    5.7341e+01,
    8.5175e+01,
    1.1416e+02,
    -1.6435e+01,
    1.1514e+01,
    4.0618e+01,
    7.0876e+01,
    1.0229e+02,
    1.3486e+02,
    -1.2253e+01,
    1.9275e+01,
    5.1958e+01,
    8.5796e+01,
    1.2079e+02,
    1.5694e+02,
    -3.9596e+01,
    -5.0367e+00,
    3.0678e+01,
    6.7547e+01,
    1.0557e+02,
    1.4475e+02,
    -3.5935e+01,
    2.2046e+00,
    4.1499e+01,
    8.1948e+01,
    1.2355e+02,
    1.6631e+02,
    -3.0887e+01,
    1.0832e+01,
    5.3705e+01,
    9.7734e+01,
    1.4292e+02,
    1.8926e+02,
    -2.4454e+01,
    2.0844e+01,
    6.7298e+01,
    1.1491e+02,
    1.6367e+02,
    2.1359e+02,
    -1.6636e+01,
    3.2243e+01,
    8.2276e+01,
    1.3346e+02,
    1.8581e+02,
    2.3930e+02,
    -5.6796e+01,
    -4.8868e+00,
    4.8178e+01,
    1.0240e+02,
    1.5777e+02,
    2.1430e+02,
    -4.9497e+01,
    5.9920e+00,
    6.2636e+01,
    1.2043e+02,
    1.7939e+02,
    2.3950e+02,
    -4.0813e+01,
    1.8256e+01,
    7.8480e+01,
    1.3986e+02,
    2.0239e+02,
    2.6608e+02,
    -3.0742e+01,
    3.1907e+01,
    9.5710e+01,
    1.6067e+02,
    2.2678e+02,
    2.9405e+02,
    -1.9286e+01,
    4.6942e+01,
    1.1433e+02,
    1.8286e+02,
    2.5256e+02,
    3.2340e+02,
];

static LAYER_NORM_BACKWARD_DATA_EXPECTED_DATA: [f32; 360] = [
    -37.4466, -40.1823, -42.8987, -45.5959, -48.2738, -50.9325, -37.6492, -40.2694, -42.8704,
    -45.4521, -48.0146, -50.5578, -37.1590, -39.6638, -42.1492, -44.6155, -47.0625, -49.4902,
    -35.9760, -38.3652, -40.7352, -43.0860, -45.4175, -47.7298, -34.1001, -36.3739, -38.6284,
    -40.8637, -43.0797, -45.2765, -28.9233, -31.0816, -33.2207, -35.3405, -37.4411, -39.5224,
    -25.6617, -27.7046, -29.7281, -31.7325, -33.7176, -35.6834, -21.7073, -23.6347, -25.5428,
    -27.4316, -29.3013, -31.1516, -17.0600, -18.8719, -20.6645, -22.4379, -24.1921, -25.9270,
    -11.7199, -13.4163, -15.0935, -16.7514, -18.3901, -20.0095, -3.0789, -4.6598, -6.2215, -7.7640,
    -9.2872, -10.7911, 3.6469, 2.1814, 0.7352, -0.6918, -2.0995, -3.4880, 11.0655, 9.7156, 8.3848,
    7.0733, 5.7811, 4.5081, 19.1770, 17.9425, 16.7273, 15.5312, 14.3545, 13.1969, 27.9814, 26.8623,
    25.7626, 24.6820, 23.6207, 22.5786, 40.0866, 39.0830, 38.0987, 37.1336, 36.1878, 35.2612,
    50.2766, 49.3885, 48.5197, 47.6701, 46.8397, 46.0286, 61.1595, 60.3869, 59.6335, 58.8994,
    58.1845, 57.4889, 72.7352, 72.0781, 71.4402, 70.8215, 70.2221, 69.6420, 85.0038, 84.4621,
    83.9397, 83.4365, 82.9526, 82.4879, -97.9178, -102.2671, -106.5971, -110.9080, -115.1995,
    -119.4718, -91.7457, -95.9795, -100.1941, -104.3894, -108.5655, -112.7223, -84.8807, -88.9990,
    -93.0981, -97.1780, -101.2386, -105.2800, -77.3229, -81.3257, -85.3094, -89.2737, -93.2189,
    -97.1448, -69.0722, -72.9596, -76.8278, -80.6767, -84.5063, -88.3167, -54.8907, -58.6626,
    -62.4153, -66.1488, -69.8629, -73.5579, -45.2544, -48.9108, -52.5480, -56.1660, -59.7647,
    -63.3441, -34.9252, -38.4661, -41.9879, -45.4903, -48.9736, -52.4376, -23.9031, -27.3286,
    -30.7349, -34.1219, -37.4896, -40.8382, -12.1882, -15.4983, -18.7890, -22.0606, -25.3129,
    -28.5459, 5.4575, 2.2629, -0.9124, -4.0684, -7.2053, -10.3228, 18.5581, 15.4790, 12.4191,
    9.3786, 6.3572, 3.3551, 32.3515, 29.3879, 26.4435, 23.5184, 20.6125, 17.7259, 46.8378, 43.9896,
    41.1608, 38.3511, 35.5607, 32.7895, 62.0169, 59.2842, 56.5708, 53.8766, 51.2017, 48.5460,
    83.1268, 80.5096, 77.9117, 75.3330, 72.7735, 70.2333, 99.6916, 97.1899, 94.7074, 92.2442,
    89.8002, 87.3755, 116.9492, 114.5630, 112.1960, 109.8483, 107.5198, 105.2105, 134.8997,
    132.6290, 130.3774, 128.1452, 125.9321, 123.7384, 153.5431, 151.3878, 149.2517, 147.1349,
    145.0374, 142.9591, -158.3889, -164.3519, -170.2956, -176.2200, -182.1252, -188.0112,
    -145.8420, -151.6895, -157.5177, -163.3267, -169.1164, -174.8869, -132.6023, -138.3343,
    -144.0470, -149.7405, -155.4148, -161.0698, -118.6697, -124.2862, -129.8835, -135.4615,
    -141.0203, -146.5598, -104.0443, -109.5453, -115.0271, -120.4897, -125.9330, -131.3570,
    -80.8580, -86.2436, -91.6099, -96.9570, -102.2848, -107.5934, -64.8469, -70.1170, -75.3679,
    -80.5995, -85.8118, -91.0049, -48.1429, -53.2976, -58.4329, -63.5491, -68.6460, -73.7236,
    -30.7461, -35.7853, -40.8052, -45.8058, -50.7873, -55.7494, -12.6565, -17.5802, -22.4846,
    -27.3698, -32.2357, -37.0824, 13.9940, 9.1858, 4.3968, -0.3729, -5.1234, -9.8546, 33.4693,
    28.7766, 24.1031, 19.4489, 14.8139, 10.1981, 53.6375, 49.0602, 44.5022, 39.9635, 35.4440,
    30.9437, 74.4985, 70.0368, 65.5942, 61.1709, 56.7669, 52.3821, 96.0524, 91.7061, 87.3790,
    83.0712, 78.7827, 74.5133, 126.1671, 121.9362, 117.7247, 113.5323, 109.3593, 105.2054,
    149.1067, 144.9913, 140.8952, 136.8183, 132.7607, 128.7223, 172.7390, 168.7392, 164.7585,
    160.7971, 156.8550, 152.9321, 197.0643, 193.1799, 189.3147, 185.4688, 181.6421, 177.8347,
    222.0824, 218.3135, 214.5638, 210.8333, 207.1221, 203.4302,
];

static LAYER_NORM_BACKWARD_GAMMA_EXPECTED_DATA: [f32; 120] = [
    -618.3635, -413.9312, -206.0345, 5.3263, 220.1513, 438.4406, -605.6325, -390.4611, -171.8253,
    50.2746, 275.8387, 504.8671, -588.7444, -362.8339, -133.4591, 99.3799, 335.6832, 575.4506,
    -567.6993, -331.0496, -90.9357, 152.6424, 399.6846, 650.1912, -542.4971, -295.1083, -44.2554,
    210.0618, 467.8433, 729.0889, -759.9636, -503.4813, -243.5347, 19.8760, 286.7511, 557.0902,
    -736.3203, -469.0988, -198.4132, 75.7366, 353.3507, 634.4291, -708.5199, -430.5594, -149.1347,
    135.7543, 424.1074, 715.9248, -676.5624, -387.8629, -95.6991, 199.9290, 499.0213, 801.5777,
    -640.4480, -341.0092, -38.1063, 268.2607, 578.0921, 891.3877, -896.3673, -587.8350, -275.8386,
    39.6221, 358.5471, 680.9362, -861.8117, -542.5403, -219.8048, 106.3950, 436.0590, 769.1873,
    -823.0990, -493.0886, -159.6139, 177.3249, 517.7281, 861.5953, -780.2293, -439.4797, -95.2660,
    252.4119, 603.5541, 958.1606, -733.2025, -381.7139, -26.7610, 331.6561, 693.5372, 1058.8828,
    -1027.5747, -666.9926, -302.9461, 64.5646, 435.5393, 809.9785, -982.1068, -610.7855, -236.0000,
    142.2497, 523.9637, 909.1418, -932.4818, -550.4214, -164.8969, 224.0920, 616.5450, 1012.4623,
    -878.6998, -485.9003, -89.6366, 310.0913, 713.2833, 1119.9397, -820.7606, -417.2221, -10.2193,
    400.2476, 814.1789, 1231.5742,
];

static LAYER_NORM_BACKWARD_BETA_EXPECTED_DATA: [f32; 120] = [
    360., 363., 366., 369., 372., 375., 378., 381., 384., 387., 390., 393., 396., 399., 402., 405.,
    408., 411., 414., 417., 420., 423., 426., 429., 432., 435., 438., 441., 444., 447., 450., 453.,
    456., 459., 462., 465., 468., 471., 474., 477., 480., 483., 486., 489., 492., 495., 498., 501.,
    504., 507., 510., 513., 516., 519., 522., 525., 528., 531., 534., 537., 540., 543., 546., 549.,
    552., 555., 558., 561., 564., 567., 570., 573., 576., 579., 582., 585., 588., 591., 594., 597.,
    600., 603., 606., 609., 612., 615., 618., 621., 624., 627., 630., 633., 636., 639., 642., 645.,
    648., 651., 654., 657., 660., 663., 666., 669., 672., 675., 678., 681., 684., 687., 690., 693.,
    696., 699., 702., 705., 708., 711., 714., 717.,
];

fn test_layer_norm2d<B: Backend>(device: Device<B>) -> Result<()> {
    let n = 3;
    let c = 4;
    let h = 5;
    let w = 6;
    let x = arange_with_shape(&[w, h, c, n], device);
    let x = x.reversed_axes()?;
    let gamma = arange_with_shape(&[c, h, w], device);
    let beta = arange_with_shape(&[c, h, w], device);
    let y = layer_norm(&x, &gamma, Some(&beta), &vec![c, h, w], 1e-7)?;
    let expected_y = Tensor::from_vec(
        LAYER_NORM_FORWARD_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&y, &expected_y, 1e-2);
    Ok(())
}

define_test!(
    test_layer_norm2d,
    test_layer_norm2d_cpu,
    test_layer_norm2d_cuda
);

fn test_layer_norm_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let n = 3;
    let c = 4;
    let h = 5;
    let w = 6;
    let x = arange_with_shape(&[w, h, c, n], device);
    let x = x.reversed_axes()?.contiguous().requires_grad();
    let gamma = arange_with_shape(&[c, h, w], device).requires_grad();
    let beta = arange_with_shape(&[c, h, w], device).requires_grad();
    let y = layer_norm(&x, &gamma, Some(&beta), &vec![c, h, w], 1e-7)?;
    let expected_y = Tensor::from_vec(
        LAYER_NORM_FORWARD_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&y, &expected_y, 1e-2);

    let x2 = arange_with_shape(&[n, c, h, w], device);
    let y2 = (y * x2)?;
    let grads = y2.backward()?;
    let gx = grads.get(&x).unwrap();
    let ggamma = grads.get(&gamma).unwrap();
    let gbeta = grads.get(&beta).unwrap();
    let expected_gx = Tensor::from_vec(
        LAYER_NORM_BACKWARD_DATA_EXPECTED_DATA.to_vec(),
        vec![n, c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&gx, &expected_gx, 1e-3);
    let expected_ggamma = Tensor::from_vec(
        LAYER_NORM_BACKWARD_GAMMA_EXPECTED_DATA.to_vec(),
        vec![c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&ggamma, &expected_ggamma, 1e-3);
    let expected_gbeta = Tensor::from_vec(
        LAYER_NORM_BACKWARD_BETA_EXPECTED_DATA.to_vec(),
        vec![c, h, w],
        device,
    )?;
    assert_tensor_with_eps(&gbeta, &expected_gbeta, 1e-3);
    Ok(())
}

define_test!(
    test_layer_norm_backward,
    test_layer_norm_backward_cpu,
    test_layer_norm_backward_cuda
);
