mod test_utils;

use rust_dnn_core::{backend::Backend, device::Device, error::Result, tensor::Tensor};
use rust_dnn_nn::function::{generate_causal_attention_mask, scaled_dot_product_attention};

use crate::test_utils::{assert_tensor, assert_tensor_with_eps};

static SCALED_DOT_PRODUCT_ATTENTION_INPUT_Q: [f32; 120] = [
    1.9269, 1.4873, 0.9007, -2.1055, 0.6784, -1.2345, -0.0431, -1.6047, -0.7521, 1.6487, -0.3925,
    -1.4036, -0.7279, -0.5594, -0.7688, 0.7624, 1.6423, -0.1596, -0.4974, 0.4396, -0.7581, 1.0783,
    0.8008, 1.6806, 1.2791, 1.2964, 0.6105, 1.3347, -0.2316, 0.0418, -0.2516, 0.8599, -1.3847,
    -0.8712, -0.2234, 1.7174, 0.3189, -0.4245, 0.3057, -0.7746, -1.5576, 0.9956, -0.8798, -0.6011,
    -1.2742, 2.1228, -1.2347, -0.4879, -0.9138, -0.6581, 0.0780, 0.5258, -0.4880, 1.1914, -0.8140,
    -0.7360, -1.4032, 0.0360, -0.0635, 0.6756, -0.0978, 1.8446, -1.1845, 1.3835, 1.4451, 0.8564,
    2.2181, 0.5232, 0.3466, -0.1973, -1.0546, 1.2780, -0.1722, 0.5238, 0.0566, 0.4263, 0.5750,
    -0.6417, -2.2064, -0.7508, 0.0109, -0.3387, -1.3407, -0.5854, 0.5362, 0.5246, 1.1412, 0.0516,
    0.7440, -0.4816, -1.0495, 0.6039, -1.7223, -0.8278, 1.3347, 0.4835, -2.5095, 0.4880, 0.7846,
    0.0286, 0.6408, 0.5832, 1.0669, -0.4502, 1.0311, -0.7048, 1.0131, -0.3308, 0.5177, 0.3878,
    -0.5797, -0.1691, -0.5733, 0.5069, -0.4752, -0.4920, 0.2704, -0.5628, 0.6793, 0.4405,
];

static SCALED_DOT_PRODUCT_ATTENTION_INPUT_K: [f32; 120] = [
    -0.3609, -0.0606, 0.0733, 0.8187, 1.4805, 0.3449, -1.4241, -0.1163, 0.2176, -0.0467, -1.4335,
    -0.5665, -0.4253, 0.2625, -1.4391, 0.5214, 1.0414, -0.3997, -2.2933, 0.4976, -0.4257, -1.3371,
    -0.1933, 0.6526, -0.3063, -0.3302, -0.9808, 0.1947, -1.6535, 0.6814, 1.4611, -0.3098, 0.9633,
    -0.3095, 0.5712, 1.1179, -1.2956, 0.0503, -0.5855, -0.3900, 0.9812, -0.6401, -0.4908, 0.2080,
    -1.1586, -0.9637, -0.3750, 0.8033, 0.7165, 1.5335, -1.4510, -0.7861, -0.9563, -1.2476, -0.7499,
    -0.5922, -1.5326, -0.7251, 0.4664, 0.6667, -0.0439, 0.2368, -0.7061, -0.7169, -0.1593, -0.4249,
    0.9442, -0.1849, 1.0608, 0.2083, -0.5778, 0.3255, 0.2618, -0.7599, -2.0461, -1.5295, 0.4049,
    0.6319, 0.3125, -0.0335, 1.3032, 0.4879, 1.1340, -0.3556, 0.3618, 1.9993, 0.6630, 0.7047,
    0.0213, -0.8293, -1.0809, -0.7839, 0.5071, 0.0821, 0.4440, -0.7240, -0.4611, -0.0639, -1.3667,
    0.3298, -0.9827, 0.3018, 0.1787, -0.1293, -0.6855, 0.5636, -1.5072, -1.6107, -1.4790, 0.4323,
    -0.1250, 0.7821, -1.5988, -0.1091, 0.7152, 0.0391, 1.3059, 0.2466, -1.9776, 0.0179,
];

static SCALED_DOT_PRODUCT_ATTENTION_INPUT_V: [f32; 120] = [
    -1.3793, 0.6258, -2.5850, -0.0240, -0.1222, -0.7470, 1.7093, 0.0579, 1.1930, 1.9373, 0.7287,
    0.9809, 0.4146, 1.1566, 0.2691, -0.0366, 0.9733, -1.0151, -0.5419, -0.4410, -0.3136, -0.1293,
    -0.7150, -0.0476, 2.0207, 0.2539, 0.9364, 0.7122, -0.0318, 0.1016, 1.3433, 0.7133, 0.4038,
    -0.7140, 0.8337, -0.9585, 0.4536, 1.2461, -2.3065, -1.2869, 0.1799, -2.1268, -0.1341, -1.0408,
    -0.7647, -0.0553, 1.2049, -0.9825, 0.4334, -0.7172, 1.0554, -1.4534, 0.4652, 0.3714, -0.0047,
    0.0795, 0.3782, 0.7051, -1.7237, -0.8435, 0.4351, 0.2659, -0.5871, 0.0827, 0.8854, 0.1824,
    0.7864, -0.0579, 0.5667, -0.7098, -0.4875, 0.0501, 0.6084, 1.6309, -0.0847, 1.0844, 0.9478,
    -0.6766, -0.5730, -0.3303, -0.7939, 0.3752, 0.0879, -1.2415, -0.3203, -0.8444, -0.5513, 1.9890,
    1.9003, 1.6951, 0.0281, -0.1754, -1.7735, -0.7046, -0.3947, 1.8868, -0.2184, 0.1663, 2.1442,
    1.7046, 0.3459, 0.6425, -0.2040, 0.6854, -1.2311, 0.8657, -1.4236, -0.6961, -0.3182, 1.2154,
    1.4200, -0.0547, 1.4862, -1.4091, -0.3639, -0.0993, 0.3105, 0.3715, 0.2697, 0.7900,
];

static SCALED_DOT_PRODUCT_ATTENTION_FORWARD_EXPECTED_DATA: [f32; 120] = [
    -0.0698, 0.9773, -1.0249, -0.5021, -0.3987, -0.4888, 0.9340, -1.2636, 0.0508, 0.0263, -0.0172,
    1.1963, -0.1038, 0.8781, 0.7135, -0.2378, 0.9660, -1.1149, -0.2878, -0.2237, 0.3557, 0.4123,
    0.1146, -0.5103, 0.9668, 0.5964, 0.6291, 0.5507, -0.9281, 0.3609, 0.0182, 0.5701, 0.5053,
    -0.6918, 0.2678, 0.0946, 0.5041, 0.6013, -1.1983, 0.1018, 0.8284, -1.2439, 0.3440, 0.0623,
    -0.1878, 0.2765, -1.5932, 0.0428, -0.9099, -0.6730, 0.2723, -0.9252, 0.0094, -0.6969, -0.6411,
    0.2705, 0.0018, 0.0932, -0.5655, -0.6096, 0.3438, 0.7265, -0.2098, 0.3467, -0.4261, 0.2389,
    0.5631, -0.1111, 0.5157, -0.2310, 0.4214, 0.6534, -0.2368, 0.2774, -0.2617, -0.0027, 0.2313,
    0.0427, 0.8503, 0.2226, 0.6550, -0.1605, -0.0789, 0.8914, 0.8766, -0.5349, -0.2224, 0.9377,
    0.7626, 0.9136, 1.0594, -0.1693, -0.2882, 1.0877, 0.9725, 0.0648, -0.1501, -0.4153, 0.1806,
    0.3729, 0.3656, 0.0604, 0.3369, -0.0436, 0.3428, 0.7457, 0.0990, 0.6001, -0.3803, -0.2060,
    0.7334, -0.0959, 0.2665, -0.2367, -0.1300, 0.8485, -0.0804, 0.5311, -0.4728, -0.1286,
];

static SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GQ_EXPECTED_DATA: [f32; 120] = [
    3.3786e-04,
    -6.8862e-02,
    -5.8850e-04,
    3.4485e-02,
    -5.0508e-02,
    -1.4454e-01,
    -5.2057e-01,
    -1.4123e-01,
    -1.0081e-01,
    -1.0331e+00,
    -2.1579e-01,
    -6.1280e-01,
    -4.2014e-02,
    3.7346e-01,
    -6.8901e-01,
    -5.3438e-02,
    -2.8579e-01,
    -4.4510e-02,
    4.0705e-02,
    -4.1336e-01,
    1.4515e-01,
    2.2919e-01,
    2.2370e-01,
    -9.3582e-02,
    2.4555e-01,
    7.9717e-02,
    3.7082e-01,
    3.3950e-01,
    3.9157e-02,
    3.9315e-01,
    -3.1134e-01,
    2.0532e-01,
    1.5322e-01,
    -1.9539e-01,
    3.7987e-01,
    -2.5949e-02,
    4.2802e-01,
    3.7709e-01,
    1.0098e-01,
    4.7405e-01,
    -4.6203e-01,
    6.3204e-03,
    -8.8073e-02,
    -3.2349e-01,
    1.6881e-02,
    -6.8667e-01,
    -1.0246e-01,
    -7.1376e-02,
    -2.4623e-01,
    3.3579e-01,
    -8.1243e-01,
    -4.0149e-02,
    7.7537e-02,
    -1.8997e-01,
    5.4798e-01,
    -3.1472e-01,
    1.0084e-01,
    8.2657e-02,
    -1.6611e-01,
    9.8900e-02,
    3.1221e-02,
    -5.9821e-03,
    -2.3829e-02,
    -3.2143e-02,
    -2.3962e-02,
    3.8432e-02,
    -2.8627e-02,
    -5.7247e-03,
    -1.1720e-01,
    -1.5920e-01,
    6.4457e-02,
    -1.4655e-02,
    -3.0278e-02,
    -9.1068e-02,
    -1.2995e-01,
    8.0363e-04,
    -6.9885e-03,
    3.8025e-02,
    -5.6703e-02,
    -1.6838e-01,
    -4.2542e-02,
    5.0890e-02,
    -5.7096e-01,
    -9.4510e-01,
    -2.1213e-01,
    6.2419e-01,
    2.4032e-01,
    -2.6740e-01,
    -1.4094e-02,
    -6.9008e-01,
    -3.7580e-02,
    8.4009e-02,
    -5.7402e-01,
    -1.1206e+00,
    -1.1310e-01,
    7.9739e-01,
    4.2209e-01,
    -2.9712e-01,
    -6.1841e-01,
    -4.5661e-01,
    1.0808e-02,
    2.9681e-01,
    1.6132e-01,
    -1.4216e-01,
    3.8833e-03,
    4.5198e-02,
    1.8132e-01,
    1.2874e-02,
    -8.2269e-02,
    6.2489e-02,
    -1.5825e-02,
    2.6989e-01,
    4.2238e-02,
    7.8284e-03,
    4.3308e-02,
    -5.9382e-03,
    2.4818e-01,
    2.2195e-02,
    6.0708e-03,
    5.2018e-02,
];

static SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GK_EXPECTED_DATA: [f32; 120] = [
    0.4469, 0.0091, 0.8536, 0.5698, -0.6355, -0.2229, -0.0568, -0.5426, -0.4421, 0.3103, -0.2957,
    -0.1125, -0.5398, -0.3479, 0.3077, 0.0717, 0.1601, 0.2287, 0.2202, 0.0175, 0.1270, -0.0743,
    -0.1229, -0.1398, -0.1501, 0.0793, 0.2942, -0.2587, -0.1204, -0.0469, 0.9847, 0.7008, 0.2929,
    0.2874, -0.0611, -1.1910, -0.9207, 0.0886, -0.0272, 0.2581, -0.3577, 0.1755, 0.4933, -0.0478,
    0.6713, 0.0102, -0.0920, -0.0814, 0.1276, -0.0749, 0.0037, -0.1071, -0.3730, -0.0930, -0.4719,
    0.3438, 0.0236, -0.0390, 0.0132, -0.1245, -0.0278, 0.0430, 0.0089, 0.1298, 0.0570, -0.0251,
    -0.1212, -0.0045, 0.0042, 0.0116, 0.0329, 0.3108, -0.0386, -0.1145, -0.0597, 0.0200, -0.2325,
    0.0342, -0.0195, -0.0088, -0.1514, -0.1650, 0.5730, -0.1754, -0.1533, 0.4268, -0.2059, -0.0157,
    0.5832, -0.1623, 0.2899, 1.1263, 1.6131, 0.2436, -1.1271, -0.5653, -0.7555, -2.1703, -0.6515,
    1.4428, 0.0673, -0.1133, 0.0193, -0.0699, -0.0863, 0.0729, -0.1035, 0.0245, -0.0898, -0.0992,
    -0.1077, 0.0509, -0.0964, 0.1089, 0.0157, -0.0324, 0.1659, 0.0527, 0.0508, 0.1697,
];

static SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GV_EXPECTED_DATA: [f32; 120] = [
    0.5922, 0.5922, 0.5922, 0.5922, 0.5922, 0.5182, 0.5182, 0.5182, 0.5182, 0.5182, 0.6482, 0.6482,
    0.6482, 0.6482, 0.6482, 2.2414, 2.2414, 2.2414, 2.2414, 2.2414, 0.7834, 0.7834, 0.7834, 0.7834,
    0.7834, 0.7577, 0.7577, 0.7577, 0.7577, 0.7577, 1.5098, 1.5098, 1.5098, 1.5098, 1.5098, 0.9492,
    0.9492, 0.9492, 0.9492, 0.9492, 1.3313, 1.3313, 1.3313, 1.3313, 1.3313, 0.5345, 0.5345, 0.5345,
    0.5345, 0.5345, 1.2995, 1.2995, 1.2995, 1.2995, 1.2995, 0.8347, 0.8347, 0.8347, 0.8347, 0.8347,
    0.8046, 0.8046, 0.8046, 0.8046, 0.8046, 1.5232, 1.5232, 1.5232, 1.5232, 1.5232, 0.9159, 0.9159,
    0.9159, 0.9159, 0.9159, 0.7562, 0.7562, 0.7562, 0.7562, 0.7562, 0.7085, 0.7085, 0.7085, 0.7085,
    0.7085, 0.8863, 0.8863, 0.8863, 0.8863, 0.8863, 1.0670, 1.0670, 1.0670, 1.0670, 1.0670, 1.3383,
    1.3383, 1.3383, 1.3383, 1.3383, 0.9913, 0.9913, 0.9913, 0.9913, 0.9913, 0.6333, 0.6333, 0.6333,
    0.6333, 0.6333, 1.3414, 1.3414, 1.3414, 1.3414, 1.3414, 1.0341, 1.0341, 1.0341, 1.0341, 1.0341,
];

static SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_FORWARD_EXPECTED_DATA: [f32; 120] = [
    -1.3793e+00,
    6.2580e-01,
    -2.5850e+00,
    -2.4000e-02,
    -1.2219e-01,
    -1.2274e+00,
    8.8614e-01,
    -1.9499e+00,
    2.6840e-01,
    3.7264e-01,
    -1.4634e-02,
    1.2261e+00,
    1.7707e-02,
    1.0674e+00,
    8.6736e-01,
    -2.3782e-01,
    9.6604e-01,
    -1.1149e+00,
    -2.8779e-01,
    -2.2369e-01,
    -3.1362e-01,
    -1.2925e-01,
    -7.1496e-01,
    -4.7562e-02,
    2.0207e+00,
    5.8193e-02,
    5.6891e-01,
    2.2004e-01,
    -3.7213e-02,
    7.6348e-01,
    3.2933e-01,
    6.0724e-01,
    2.6938e-01,
    -1.7753e-01,
    7.6298e-01,
    9.4610e-02,
    5.0413e-01,
    6.0127e-01,
    -1.1983e+00,
    1.0180e-01,
    1.7989e-01,
    -2.1268e+00,
    -1.3408e-01,
    -1.0408e+00,
    -7.6472e-01,
    1.7132e-01,
    -2.0054e+00,
    -1.6499e-01,
    -9.8706e-01,
    -7.6299e-01,
    3.2461e-01,
    -1.2786e+00,
    -1.7924e-01,
    -4.1848e-01,
    -5.8627e-01,
    2.7050e-01,
    1.7960e-03,
    9.3236e-02,
    -5.6553e-01,
    -6.0956e-01,
    4.3514e-01,
    2.6589e-01,
    -5.8710e-01,
    8.2689e-02,
    8.8538e-01,
    2.5441e-01,
    6.3815e-01,
    -2.0862e-01,
    4.2884e-01,
    -2.5549e-01,
    7.0159e-02,
    4.9746e-01,
    -3.8566e-03,
    7.2791e-01,
    -2.2534e-01,
    -2.6536e-03,
    2.3132e-01,
    4.2727e-02,
    8.5033e-01,
    2.2262e-01,
    -7.9394e-01,
    3.7523e-01,
    8.7910e-02,
    -1.2415e+00,
    -3.2025e-01,
    -8.2679e-01,
    -2.2824e-01,
    1.3260e+00,
    8.0473e-01,
    9.9231e-01,
    -2.3629e-01,
    -9.2303e-02,
    -9.9991e-01,
    -5.6650e-01,
    -1.7387e-01,
    6.4786e-02,
    -1.5009e-01,
    -4.1526e-01,
    1.8056e-01,
    3.7290e-01,
    3.4590e-01,
    6.4248e-01,
    -2.0395e-01,
    6.8537e-01,
    -1.2311e+00,
    4.6988e-01,
    1.4972e-01,
    -3.2133e-01,
    4.4600e-01,
    -6.4756e-01,
    8.5948e-01,
    -1.5745e-01,
    2.5057e-01,
    -3.1339e-01,
    -2.6934e-01,
    8.4846e-01,
    -8.0359e-02,
    5.3107e-01,
    -4.7279e-01,
    -1.2860e-01,
];

static SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GQ_EXPECTED_DATA: [f32; 120] = [
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    4.3994e-01,
    -8.4988e-01,
    -1.1816e-01,
    -3.7466e-01,
    -9.5185e-01,
    1.9275e-02,
    -2.7469e-01,
    -7.7631e-02,
    -1.5177e-01,
    -5.2436e-01,
    -5.3438e-02,
    -2.8579e-01,
    -4.4510e-02,
    4.0705e-02,
    -4.1336e-01,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    1.1173e-02,
    4.1663e-02,
    4.5372e-02,
    -2.6963e-01,
    1.1548e-01,
    1.4219e-01,
    9.0407e-02,
    1.0122e-01,
    -1.6884e-01,
    1.0638e-01,
    -2.5949e-02,
    4.2802e-01,
    3.7709e-01,
    1.0098e-01,
    4.7405e-01,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    -1.1510e-01,
    1.5690e-02,
    7.6583e-02,
    3.0094e-02,
    1.5931e-01,
    -9.8889e-01,
    1.3050e-02,
    1.2673e-01,
    -2.7467e-01,
    6.1469e-01,
    -3.1472e-01,
    1.0084e-01,
    8.2657e-02,
    -1.6611e-01,
    9.8900e-02,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    1.0906e-02,
    -2.0246e-02,
    -1.4915e-02,
    -5.0877e-02,
    -1.0522e-02,
    -1.0386e-02,
    -4.5299e-02,
    3.1203e-02,
    -1.3285e-01,
    -1.6183e-01,
    8.0363e-04,
    -6.9885e-03,
    3.8025e-02,
    -5.6703e-02,
    -1.6838e-01,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    4.2997e-01,
    1.0817e-01,
    -2.6513e-01,
    2.3275e-01,
    -7.3569e-01,
    8.8479e-01,
    4.1822e-01,
    6.4907e-02,
    -2.4172e-02,
    -3.5023e-01,
    7.9739e-01,
    4.2209e-01,
    -2.9712e-01,
    -6.1841e-01,
    -4.5661e-01,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    0.0000e+00,
    -7.4787e-02,
    8.7492e-02,
    8.6545e-02,
    6.5282e-02,
    -5.4062e-02,
    -4.1155e-02,
    2.1157e-01,
    -3.8829e-02,
    1.2061e-01,
    5.6235e-02,
    -5.9382e-03,
    2.4818e-01,
    2.2195e-02,
    6.0708e-03,
    5.2018e-02,
];

static SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GK_EXPECTED_DATA: [f32; 120] = [
    7.4324e-01,
    1.1510e-01,
    1.2158e+00,
    6.9744e-01,
    -9.0204e-01,
    -7.3491e-01,
    -4.5739e-02,
    -1.1429e+00,
    -6.2776e-01,
    9.5682e-01,
    3.3435e-02,
    2.0605e-02,
    -8.1608e-02,
    -9.6922e-02,
    -3.0696e-02,
    -4.1768e-02,
    -8.9968e-02,
    8.7430e-03,
    2.7248e-02,
    -2.4081e-02,
    -4.2468e-02,
    -1.5616e-01,
    -1.5375e-02,
    1.4226e-01,
    -1.5064e-02,
    2.5525e-01,
    1.2931e-01,
    6.8679e-02,
    -4.3894e-02,
    -5.6254e-02,
    6.9407e-01,
    1.9523e-01,
    -2.7747e-01,
    6.3066e-02,
    -3.3771e-01,
    -9.0686e-01,
    -1.6839e-01,
    2.2417e-01,
    -1.6144e-01,
    4.0902e-01,
    -7.9594e-02,
    -6.8012e-03,
    2.4188e-01,
    -4.6862e-01,
    3.2674e-01,
    7.4152e-02,
    -9.9703e-02,
    -1.1817e-01,
    1.6613e-01,
    -1.3248e-01,
    -7.3273e-02,
    -4.3577e-02,
    -1.1985e-01,
    2.9570e-01,
    -1.2201e-01,
    7.8716e-02,
    1.5008e-01,
    -3.8507e-03,
    6.7890e-03,
    -7.2259e-02,
    4.5144e-03,
    4.0436e-02,
    4.2515e-02,
    1.0624e-01,
    2.6952e-02,
    4.6514e-02,
    -1.6482e-01,
    6.5709e-03,
    -1.7304e-02,
    1.2013e-02,
    -3.9351e-02,
    1.4013e-01,
    -6.6665e-02,
    -1.4938e-01,
    -5.9532e-02,
    -1.1678e-02,
    -1.5752e-02,
    1.7579e-02,
    6.0442e-02,
    2.0567e-02,
    -4.1825e-01,
    -2.9902e-01,
    -1.3837e-01,
    -5.9736e-01,
    3.1585e-01,
    1.9487e-01,
    4.8484e-02,
    -2.8013e-01,
    4.8966e-01,
    7.7586e-02,
    6.1637e-04,
    1.4067e+00,
    1.9368e-01,
    -2.5376e-01,
    -4.0664e-01,
    2.2277e-01,
    -1.1561e+00,
    2.2482e-01,
    3.6146e-01,
    1.3198e-02,
    3.2989e-03,
    3.9398e-02,
    2.4751e-02,
    -1.9996e-02,
    7.5850e-03,
    1.2649e-01,
    -5.7007e-02,
    1.1382e-01,
    -1.2688e-01,
    -1.4285e-02,
    -1.0065e-01,
    1.5992e-03,
    -1.0525e-01,
    1.0665e-01,
    -1.9386e-02,
    -2.9135e-02,
    1.6010e-02,
    -3.3327e-02,
    4.0227e-02,
    2.6086e-02,
];

static SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GV_EXPECTED_DATA: [f32; 120] = [
    1.9922, 1.9922, 1.9922, 1.9922, 1.9922, 0.6776, 0.6776, 0.6776, 0.6776, 0.6776, 0.5795, 0.5795,
    0.5795, 0.5795, 0.5795, 0.7507, 0.7507, 0.7507, 0.7507, 0.7507, 1.7566, 1.7566, 1.7566, 1.7566,
    1.7566, 1.2619, 1.2619, 1.2619, 1.2619, 1.2619, 0.5821, 0.5821, 0.5821, 0.5821, 0.5821, 0.3995,
    0.3995, 0.3995, 0.3995, 0.3995, 2.6118, 2.6118, 2.6118, 2.6118, 2.6118, 0.5368, 0.5368, 0.5368,
    0.5368, 0.5368, 0.4493, 0.4493, 0.4493, 0.4493, 0.4493, 0.4021, 0.4021, 0.4021, 0.4021, 0.4021,
    1.8613, 1.8613, 1.8613, 1.8613, 1.8613, 1.3163, 1.3163, 1.3163, 1.3163, 1.3163, 0.7538, 0.7538,
    0.7538, 0.7538, 0.7538, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 1.7476, 1.7476, 1.7476, 1.7476,
    1.7476, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 1.1432, 1.1432, 1.1432, 1.1432, 1.1432, 0.1830,
    0.1830, 0.1830, 0.1830, 0.1830, 2.3854, 2.3854, 2.3854, 2.3854, 2.3854, 0.6875, 0.6875, 0.6875,
    0.6875, 0.6875, 0.7869, 0.7869, 0.7869, 0.7869, 0.7869, 0.1402, 0.1402, 0.1402, 0.1402, 0.1402,
];

fn test_scaled_dot_product_attention<B: Backend>(device: Device<B>) -> Result<()> {
    let q = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_Q.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let k = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_K.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let v = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_V.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let y = scaled_dot_product_attention(&q, &k, &v, None, 0.0, true, None)?;
    let expected_y = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_FORWARD_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&y, &expected_y);
    Ok(())
}

define_test!(
    test_scaled_dot_product_attention,
    test_scaled_dot_product_attention_cpu,
    test_scaled_dot_product_attention_cuda
);

fn test_scaled_dot_product_attention_mask<B: Backend>(device: Device<B>) -> Result<()> {
    let q = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_Q.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let k = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_K.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let v = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_V.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let mask = generate_causal_attention_mask(q.shape()[2], device)?;
    let y = scaled_dot_product_attention(&q, &k, &v, Some(&mask), 0.0, true, None)?;
    let expected_y = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_FORWARD_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&y, &expected_y);
    Ok(())
}

define_test!(
    test_scaled_dot_product_attention_mask,
    test_scaled_dot_product_attention_mask_cpu,
    test_scaled_dot_product_attention_mask_cuda
);

fn test_scaled_dot_product_attention_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let q = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_Q.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    let k = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_K.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    let v = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_V.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let y = scaled_dot_product_attention(&q, &k, &v, None, 0.0, true, None)?;
    let expected_y = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_FORWARD_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    assert_tensor(&y, &expected_y);

    let grads = y.backward()?;
    let gq = grads.get(&q).unwrap();
    let gk = grads.get(&k).unwrap();
    let gv = grads.get(&v).unwrap();

    let expected_gq = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GQ_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&gq, &expected_gq);

    let expected_gk = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GK_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor_with_eps(&gk, &expected_gk, 1e-3);

    let expected_gv = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_BACKWARD_GV_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&gv, &expected_gv);

    Ok(())
}

define_test!(
    test_scaled_dot_product_attention_backward,
    test_scaled_dot_product_attention_backward_cpu,
    test_scaled_dot_product_attention_backward_cuda
);

fn test_scaled_dot_product_attention_mask_backward<B: Backend>(device: Device<B>) -> Result<()> {
    let q = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_Q.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    let k = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_K.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    let v = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_INPUT_V.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    let mask = generate_causal_attention_mask(q.shape()[2], device)?;
    let y = scaled_dot_product_attention(&q, &k, &v, Some(&mask), 0.0, true, None)?;
    let expected_y = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_FORWARD_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?
    .requires_grad();
    assert_tensor(&y, &expected_y);

    let grads = y.backward()?;
    let gq = grads.get(&q).unwrap();
    let gk = grads.get(&k).unwrap();
    let gv = grads.get(&v).unwrap();

    let expected_gq = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GQ_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&gq, &expected_gq);

    let expected_gk = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GK_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor_with_eps(&gk, &expected_gk, 1e-3);

    let expected_gv = Tensor::from_vec(
        SCALED_DOT_PRODUCT_ATTENTION_CAUSAL_BACKWARD_GV_EXPECTED_DATA.to_vec(),
        vec![2, 3, 4, 5],
        device,
    )?;
    assert_tensor(&gv, &expected_gv);

    Ok(())
}

define_test!(
    test_scaled_dot_product_attention_mask_backward,
    test_scaled_dot_product_attention_mask_backward_cpu,
    test_scaled_dot_product_attention_mask_backward_cuda
);
